{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977155b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "facc2fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0., 1.], [1., 0.]], dtype=torch.complex128)\n",
    "y = torch.tensor([[0., -1.j], [1.j, 0.]], dtype=torch.complex128)\n",
    "z = torch.tensor([[1., 0.], [0., -1.]], dtype=torch.complex128)\n",
    "i2 = torch.eye(2, dtype=torch.complex128)\n",
    "\n",
    "xx = torch.kron(x, x)\n",
    "yy = torch.kron(y, y)\n",
    "rr = (xx + yy) / 2.\n",
    "rr2 = torch.matmul(rr, rr)\n",
    "d = torch.tensor([[1., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 1.]], dtype=torch.complex128)\n",
    "def Rrr(theta):\n",
    "    if isinstance(theta, torch.Tensor):\n",
    "        th = theta\n",
    "    else:\n",
    "        th = torch.tensor(theta)\n",
    "        \n",
    "    return d + torch.cos(th) * rr2 - 1.j * torch.sin(th) * rr\n",
    "\n",
    "cx = torch.tensor([[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 0., 1.], [0., 0., 1., 0.]], dtype=torch.complex128)\n",
    "\n",
    "def Rt(t, theta):\n",
    "    if isinstance(theta, torch.Tensor):\n",
    "        th2 = theta / 2.\n",
    "    else:\n",
    "        th2 = torch.tensor(theta / 2.)\n",
    "    return torch.cos(th2) * i2 - 1.j * torch.sin(th2) * t\n",
    "\n",
    "def U2(phi, lamb):\n",
    "    return torch.chain_matmul(Rt(z, phi), Rt(y, np.pi / 2.), Rt(z, lamb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "753e3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecompositionFinder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecompositionFinder, self).__init__()\n",
    "        \n",
    "        # u2-cx-rz-cx\n",
    "        # u2-+--ry-+\n",
    "        self.u2_0_0_lambda = torch.nn.Parameter(torch.empty(1))\n",
    "        self.u2_0_0_phi = torch.nn.Parameter(torch.empty(1))\n",
    "        self.u2_1_0_lambda = torch.nn.Parameter(torch.empty(1))\n",
    "        self.u2_1_0_phi = torch.nn.Parameter(torch.empty(1))\n",
    "        self.rz_0_0_phi = torch.nn.Parameter(torch.empty(1))\n",
    "        self.ry_1_0_phi = torch.nn.Parameter(torch.empty(1))\n",
    "        \n",
    "        self.ansatz = torch.chain_matmul(\n",
    "                torch.kron(U2(self.u2_0_0_phi, self.u2_0_0_lambda), torch.eye(2)),\n",
    "                torch.kron(torch.eye(2), U2(self.u2_1_0_phi, self.u2_1_0_lambda)),\n",
    "                cx,\n",
    "                torch.kron(Rt(z, self.rz_0_0_phi), torch.eye(2)),\n",
    "                torch.kron(torch.eye(2), Rt(y, self.ry_1_0_phi)),\n",
    "                cx\n",
    "        )\n",
    "        \n",
    "    def forward(self, theta):\n",
    "        return 1. - torch.abs(torch.trace(torch.matmul(Rrr(theta), self.ansatz)) / 4.)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c087f61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7512, dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor(0.7512, dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor(0.7512, dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor(0.7512, dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor(0.7512, dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor(0.7512, dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor(0.7512, dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor(0.7512, dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor(0.7512, dtype=torch.float64, grad_fn=<RsubBackward1>)\n",
      "tensor(0.7512, dtype=torch.float64, grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "model = DecompositionFinder()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "for _ in range(10):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(0.2)\n",
    "        print(pred)\n",
    "        pred.backward(retain_graph=True)\n",
    "        return pred\n",
    "        \n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d85fcb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "u2_0_0_phi = torch.empty(1, dtype=torch.float64, requires_grad=True)\n",
    "u2_0_0_lambda = torch.empty(1, dtype=torch.float64, requires_grad=True)\n",
    "u2_1_0_phi = torch.empty(1, dtype=torch.float64, requires_grad=True)\n",
    "u2_1_0_lambda = torch.empty(1, dtype=torch.float64, requires_grad=True)\n",
    "rz_0_0_phi = torch.empty(1, dtype=torch.float64, requires_grad=True)\n",
    "ry_1_0_phi = torch.empty(1, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "def abstrace(theta):\n",
    "    ansatz = torch.chain_matmul(\n",
    "       torch.kron(U2(u2_0_0_phi, u2_0_0_lambda), i2),\n",
    "       torch.kron(i2, U2(u2_1_0_phi, u2_1_0_lambda)),\n",
    "       cx,\n",
    "       torch.kron(Rt(z, rz_0_0_phi), i2),\n",
    "       torch.kron(i2, Rt(y, ry_1_0_phi)),\n",
    "       cx\n",
    "    )\n",
    "\n",
    "    return torch.abs(torch.trace(torch.matmul(Rrr(theta), ansatz)) / 4.)\n",
    "\n",
    "def loss_function(at):\n",
    "    return torch.tensor(1., dtype=torch.float64) - at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65bd15cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "at = abstrace(0.4)\n",
    "loss = loss_function(at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d2d4389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5213188736114618\n",
      "0.718103472611408\n",
      "0.5425749576121361\n",
      "0.5491768836469255\n",
      "0.6149893610737223\n",
      "0.570923717624815\n",
      "0.5137083906476951\n",
      "0.5285905034548213\n",
      "0.5653698640271216\n",
      "0.55834137046717\n",
      "0.5240514047047469\n",
      "0.5105037476628065\n",
      "0.5293814296540195\n",
      "0.5442733170864641\n",
      "0.5339353649824978\n",
      "0.5145304359927486\n",
      "0.5112135599592129\n",
      "0.5236787889873057\n",
      "0.5303903296967213\n",
      "0.5223100237073496\n",
      "0.5113729125842796\n",
      "0.5114525612347783\n",
      "0.5193518473488696\n",
      "0.5217793573012213\n",
      "0.5154712673581117\n",
      "0.5096643634636907\n",
      "0.5115357692714453\n",
      "0.5163241730941459\n",
      "0.5160315632699468\n",
      "0.5112775068604964\n",
      "0.5089316197681915\n",
      "0.5113416502230814\n",
      "0.5135496205361392\n",
      "0.5117741130808408\n",
      "0.5087528202489404\n",
      "0.5086418355052735\n",
      "0.5105367674367197\n",
      "0.510649521016888\n",
      "0.5085614940918004\n",
      "0.5072956201090767\n",
      "0.5081115852033993\n",
      "0.508809606779286\n",
      "0.5077477623797515\n",
      "0.5063353163167108\n",
      "0.5062997751109974\n",
      "0.5068709045371076\n",
      "0.5064466966282066\n",
      "0.5053305350354279\n",
      "0.5049101467865955\n",
      "0.5052033211394042\n",
      "0.5050610507850695\n",
      "0.5042745042523571\n",
      "0.5037698868370812\n",
      "0.5038675608314735\n",
      "0.5038390783618474\n",
      "0.5033227741948443\n",
      "0.5028818039824412\n",
      "0.5028825123713052\n",
      "0.5028935126488472\n",
      "0.5025665047928696\n",
      "0.5022290266990275\n",
      "0.5021983427939236\n",
      "0.502223274903814\n",
      "0.5020249651902997\n",
      "0.501798893165107\n",
      "0.5017828672948232\n",
      "0.5018188760032862\n",
      "0.5017048904554753\n",
      "0.5015689424891188\n",
      "0.5015761383133504\n",
      "0.501615118033012\n",
      "0.5015495705160621\n",
      "0.5014748899635867\n",
      "0.5014971562319634\n",
      "0.5015289220466246\n",
      "0.5014871715248697\n",
      "0.501450241601705\n",
      "0.5014785786877947\n",
      "0.5014997643775636\n",
      "0.5014711333229178\n",
      "0.5014575583941667\n",
      "0.5014850659590291\n",
      "0.5014952427199117\n",
      "0.5014748310474688\n",
      "0.5014739554914871\n",
      "0.5014944863708581\n",
      "0.5014942370291571\n",
      "0.5014795934489488\n",
      "0.5014840135245232\n",
      "0.5014950412650498\n",
      "0.5014878938621271\n",
      "0.5014788042736548\n",
      "0.5014844367896328\n",
      "0.5014872736930572\n",
      "0.5014782471546739\n",
      "0.5014744065983254\n",
      "0.501478570915959\n",
      "0.5014759202250125\n",
      "0.5014685941983013\n",
      "0.5014682137947195\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam([u2_0_0_phi, u2_0_0_lambda, u2_1_0_phi, u2_1_0_lambda, rz_0_0_phi, ry_1_0_phi], lr=0.6)\n",
    "#optimizer = torch.optim.Adam([u2_0_0_phi], lr=0.1)\n",
    "\n",
    "for _ in range(100):\n",
    "    at = abstrace(0.4)\n",
    "    loss = loss_function(at)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss.detach().numpy().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "723d9e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.8100000619888306\n",
      "0.640659749507904\n",
      "0.4922233521938324\n",
      "0.3647424578666687\n",
      "0.2580271363258362\n",
      "0.1715918928384781\n",
      "0.10460098832845688\n",
      "0.05582057312130928\n",
      "0.023588238283991814\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "optimizer = torch.optim.Adam([x], lr=0.1)\n",
    "def loss_fct(x):\n",
    "    return x * x\n",
    "\n",
    "for _ in range(10):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fct(x)\n",
    "        print(loss.detach().numpy().item())\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bbcc3533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "l = loss_fct(x)\n",
    "print(l.backward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e32aaaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-97b42495692b>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  l.grad\n"
     ]
    }
   ],
   "source": [
    "l.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2d0267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
